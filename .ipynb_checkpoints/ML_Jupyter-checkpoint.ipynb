{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read all CSVs with stocks data and append to one big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading/CSVs\")\n",
    "filepaths = [f for f in os.listdir(\"./\") if f.endswith('.csv')]\n",
    "df = pd.DataFrame()\n",
    "for i in filepaths:\n",
    "    iterate_df = pd.DataFrame()\n",
    "    iterate_df = pd.read_csv(i)\n",
    "    iterate_df[\"stock\"] = i[:-4]\n",
    "    df = pd.concat([df,iterate_df])\n",
    "#df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Let's keep only KPI type columns that we initially considered as the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"date\",\"roe\", \"longTermDebtEquity\", \"grossMargin\", \"revenueQoQ\", \"rps\", \"epsQoQ\", \"piotroskiFScore\", \"currentRatio\", \"roa\", \"profitMargin\",\"peRatio\", \"pbRatio\",\"trailingPEG1Y\",\"VIX_high\",\"sector\",\"industry\",\"10Y_bonds\", \"10Y_bond_MoM\",\"Debt-to-Equity_Ratio\",\"DividendsYield\",\"PayoutRatio\",\"Acc_Rec_Pay_Ration\",\"Earnings_per_stock\",\"dividends_change\",\"prev_div_change\",\"days_after_divid_report\",\"surprise_%\", \"expected_growth\", \"previous_surprise\",\"days_after_earn_report\",\"future_30dprice_change\",\"stock\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up Date column as an index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index([\"date\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The column future_30dprice_change is our target column\n",
    "###We do not need rows with nan values there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.future_30dprice_change.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Let's check how much empty values we have by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###There are 691 empty values related to dividends.\n",
    "### As this dataframe consist of data from 30 companies, it shows one company in the list doesn't pay dividends.\n",
    "### The stock is CRM\n",
    "###We should not remove this data, it's better to replace it with 0's \n",
    "###as there are many companies that do not pay dividends we may analyze in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"dividends_change\"].isna()].stock\n",
    "\n",
    "df.dividends_change = df.dividends_change.fillna(0)\n",
    "df.prev_div_change = df.prev_div_change.fillna(0)\n",
    "df.days_after_divid_report = df.days_after_divid_report.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can drop rows where 10Y_bond_MoM is nan. Those are the earliest days in the dataset.\n",
    "###We had a limit of 1450 days of historical data creating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['10Y_bond_MoM'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We can also drop rows with the blank previous surprise. \n",
    "#We are not loosing much data and it is related to limit of historical numbers in calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['previous_surprise'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for several stocks currentRatio is not available.\n",
    "### this KPI shows short term debt to cash ratio\n",
    "### We should not remove it as we will loose data for a full stock.\n",
    "### let's replace it with average values\n",
    "### but first we need to split our dataset to train and test to avoid contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now we can deal with CurrentRatio\n",
    "###First let's replace nan by mean in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['currentRatio'] = df_train['currentRatio'].fillna(df_train.groupby('sector')['currentRatio'].transform('mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now let's use the same values from training data to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stocks_empty_ratio = df_test[df_test[\"currentRatio\"].isna()].stock.unique()\n",
    "for paper in list_of_stocks_empty_ratio:\n",
    "    df_test.loc[df_test.stock == paper,'currentRatio'] = df_train[df_train.stock == paper]['currentRatio'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#find the highly correlated columns so we can remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df, size=11):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    for (i, j), z in np.ndenumerate(corr):\n",
    "        ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
    "        \n",
    "plot_corr(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We have 4 columns with correlation higher 0.95\n",
    "###longTermDebtEquity correlates with pbRatio\n",
    "###grossMargin correlates with profitMargin\n",
    "### out of definition of those we understand that they are very close and we can drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop([\"longTermDebtEquity\",\"grossMargin\"], axis = 1, inplace = True)\n",
    "df_test.drop([\"longTermDebtEquity\",\"grossMargin\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Let's check if there are variables with correlation above 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= df_train.corr().replace(1,np.nan)\n",
    "corr = corr[corr>0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are no columns with correlation above 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we better t oseparate numeric columns to check the distribution and make a deeper analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = df_train.select_dtypes([np.number]).columns\n",
    "cols_str = df_train.select_dtypes('object').columns\n",
    "\n",
    "df_train[cols_num].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
