{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_core.paths import jupyter_path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb # Our ML library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all CSVs with stocks data and append to one big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading/CSVs\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading/SP500_CSVs_01032023\")\n",
    "filepaths = [f for f in os.listdir(\"./\") if f.endswith('.csv')]\n",
    "df = pd.DataFrame()\n",
    "for i in filepaths:\n",
    "    iterate_df = pd.DataFrame()\n",
    "    iterate_df = pd.read_csv(i, encoding= 'unicode_escape')\n",
    "    iterate_df[\"stock\"] = i[:-4]\n",
    "    df = pd.concat([df,iterate_df])\n",
    "#df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "df = df[df.close.notna()]\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading/ML_part/EOD\")\n",
    "df.to_csv(\"THE_FINAL_DATASET_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record the dataframe to speed up the future reading process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/\")\n",
    "df.to_csv(\"THE_FINAL_DATASET_2023.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/ML_part/EOD\")\n",
    "df= pd.read_csv(\"THE_FINAL_DATASET_2023.csv\")\n",
    "#df.drop([\"Unnamed: 0\",\"dataCode\",\"name\",\"description\",\"statementType\",\"units\",\"Unnamed: 0.2\", \"Unnamed: 0.1\"], axis = 1, inplace = True)\n",
    "df = df[df.close.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['YoY_DY'] = 100*df['DY'].pct_change(periods = -365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702978, 65)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Date column as an index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much empty values we have by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_after_earnings_report     1420\n",
      "LTDE                           7610\n",
      "DE                             1182\n",
      "DPR                           66581\n",
      "Acc_Rec_Pay_Ration             1999\n",
      "DY                            66581\n",
      "PEG_Forward                    2941\n",
      "EPS_surprise                   1420\n",
      "EPS_YoY_Growth                 1420\n",
      "EPS_QoQ_frcst_diff             1420\n",
      "EPS_1Y_exp_Change              4361\n",
      "YoY_DPR                       61477\n",
      "YoY_AR_Ration                  1483\n",
      "YoY_DY                        61477\n",
      "EPS_1Y_exp_Change_QoQ         52830\n",
      "future_15dprice_change         5390\n",
      "future_30dprice_change        10780\n",
      "future_60dprice_change        21560\n",
      "future_90dprice_change        32340\n",
      "future_120dprice_change       43113\n",
      "future_150dprice_change       53852\n",
      "VIX_DoD                         490\n",
      "VIX_WoW                        2450\n",
      "VIX_MoM                       10780\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "zeroes = df.isnull().sum()\n",
    "print(zeroes[zeroes>0])\n",
    "del zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with \"EPS\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many null values in the \"current_ratio\" column. Let's see the stocks where it happens and then decide what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['APA', 'DD', 'ETSY', 'FOX', 'HSIC', 'INVH', 'L', 'NI', 'PODD'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.EPS_surprise.isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the original files, it's apparently the null values comes from the older data.\n",
    "\n",
    "Before 2017 our API did not provide us info about earinings. \n",
    "\n",
    "We can drop the reocrds with 'nan' values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will remove only 10k records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_surprise.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTDE                        7610\n",
      "DE                          1182\n",
      "DPR                        66492\n",
      "Acc_Rec_Pay_Ration          1999\n",
      "DY                         66492\n",
      "PEG_Forward                 2941\n",
      "EPS_1Y_exp_Change           2941\n",
      "YoY_DPR                    61388\n",
      "YoY_AR_Ration               1483\n",
      "YoY_DY                     61388\n",
      "EPS_1Y_exp_Change_QoQ      52830\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32181\n",
      "future_120dprice_change    42910\n",
      "future_150dprice_change    53605\n",
      "VIX_DoD                      490\n",
      "VIX_WoW                     2450\n",
      "VIX_MoM                    10780\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_null_cols(df):\n",
    "    zeroes = df.isnull().sum()\n",
    "    print(zeroes[zeroes>0])\n",
    "    \n",
    "check_null_cols(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the null values in the \"dividends\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 30% of data is affected by null values in the dividends columns.\n",
    "Let's check how many companies are in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies with zero values in dividends 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of companies with zero values in dividends\",len(df[df['DY'].isnull()].stock.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This companies do not pay dividends, we can replace payments to \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DY'].fillna(0, inplace=True)\n",
    "df['YoY_DPR'].fillna(0, inplace=True)\n",
    "df['DPR'].fillna(0, inplace=True)\n",
    "df['YoY_DY'].fillna(0, inplace=True)                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mark the companies with no dividends with the categorical columnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['DY'] == 0,\"Pays_Divds\"] = 1\n",
    "df.loc[df['DY'] != 0,\"Pays_Divds\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't pay dividends\n",
      "sector 7\n",
      "industry 14\n",
      "DPR 1\n",
      "DY 1\n",
      "Piotroski_Score 9\n",
      "YoY_DPR 1\n",
      "YoY_DY 1\n",
      "Pays_Divds 1\n",
      "\n",
      "Pay dividends\n",
      "sector 11\n",
      "Piotroski_Score 9\n",
      "Pays_Divds 1\n"
     ]
    }
   ],
   "source": [
    "temporary = df[df['DY'] == 0].copy()\n",
    "\n",
    "print(\"Doesn't pay dividends\")\n",
    "for column in temporary.columns:\n",
    "    if temporary[column].nunique() < 20:\n",
    "        print(column, temporary[column].nunique())\n",
    "        \n",
    "temporary = df[df['DY'] != 0].copy()\n",
    "\n",
    "print(\"\\nPay dividends\")\n",
    "for column in temporary.columns:\n",
    "    if temporary[column].nunique() < 20:\n",
    "        print(column, temporary[column].nunique())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with YoY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YoY variables with null values are caused by YoY calculation of the rows without historical data.\n",
    "Let's drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.YoY_CR.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTDE                        7610\n",
      "DE                          1182\n",
      "Acc_Rec_Pay_Ration          1999\n",
      "PEG_Forward                 2941\n",
      "EPS_1Y_exp_Change           2941\n",
      "YoY_AR_Ration               1483\n",
      "EPS_1Y_exp_Change_QoQ      52830\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32181\n",
      "future_120dprice_change    42910\n",
      "future_150dprice_change    53605\n",
      "VIX_DoD                      490\n",
      "VIX_WoW                     2450\n",
      "VIX_MoM                    10780\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(701558, 66)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with debt ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the companies that contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FFIV', 'FTNT'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['DE'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems these companies had zero debt for some quarters.\n",
    "\n",
    "Let's replace with \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DE'].fillna(0, inplace=True)\n",
    "df['LTDE'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc_Rec_Pay_Ration          1999\n",
      "PEG_Forward                 2941\n",
      "EPS_1Y_exp_Change           2941\n",
      "YoY_AR_Ration               1483\n",
      "EPS_1Y_exp_Change_QoQ      52830\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32181\n",
      "future_120dprice_change    42910\n",
      "future_150dprice_change    53605\n",
      "VIX_DoD                      490\n",
      "VIX_WoW                     2450\n",
      "VIX_MoM                    10780\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with VIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove null values for VIX as these are related to the historical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.VIX_MoM.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc_Rec_Pay_Ration          1933\n",
      "PEG_Forward                 2637\n",
      "EPS_1Y_exp_Change           2637\n",
      "YoY_AR_Ration               1461\n",
      "EPS_1Y_exp_Change_QoQ      42645\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32174\n",
      "future_120dprice_change    42869\n",
      "future_150dprice_change    53561\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Accounts Payable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AVB', 'CINF', 'MAA'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Acc_Rec_Pay_Ration'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems these companies had zero Accounts receivables for some quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Acc_Rec_Pay_Ration'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with PEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CZR', 'FOXA', 'KHC', 'LHX', 'PLD', 'SEDG'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['PEG_Forward'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the rows as the effect would be unsignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.PEG_Forward.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['PEG_Backwards'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.PEG_Backwards.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_1Y_exp_Change.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YoY_AR_Ration               1461\n",
      "EPS_1Y_exp_Change_QoQ      40008\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32174\n",
      "future_120dprice_change    42869\n",
      "future_150dprice_change    53561\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sector'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the sector and industry values for the GEN company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['stock'] == 'GEN','sector'] = 'Information Technology'\n",
    "df.loc[df['stock'] == 'GEN','industry'] = 'Software & Services'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YoY_AR_Ration               1461\n",
      "EPS_1Y_exp_Change_QoQ      40008\n",
      "future_15dprice_change      5357\n",
      "future_30dprice_change     10714\n",
      "future_60dprice_change     21445\n",
      "future_90dprice_change     32174\n",
      "future_120dprice_change    42869\n",
      "future_150dprice_change    53561\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.YoY_AR_Ration.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can keep only the necessery columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column sector, data: \n",
      " ['Health Care' 'Industrials' 'Consumer Discretionary'\n",
      " 'Information Technology' 'Financials' 'Consumer Staples' 'Utilities'\n",
      " 'Materials' 'Real Estate' 'Energy' 'Communication Services']\n",
      "column industry, data: \n",
      " ['Pharmaceuticals, Biotechnology & Life Sciences' 'Transportation'\n",
      " 'Retailing' 'Technology Hardware & Equipment'\n",
      " 'Health Care Equipment & Services' 'Insurance' 'Software & Services'\n",
      " 'Semiconductors & Semiconductor Equipment' 'Food, Beverage & Tobacco'\n",
      " 'Utilities' 'Materials' 'Capital Goods' 'Diversified Financials'\n",
      " 'Real Estate' 'Energy' 'Automobiles & Components' 'Media & Entertainment'\n",
      " 'Banks' 'Consumer Services' 'Household & Personal Products'\n",
      " 'Food & Staples Retailing' 'Commercial & Professional Services'\n",
      " 'Consumer Durables & Apparel' 'Telecommunication Services']\n",
      "column Piotroski_Score, data: \n",
      " [5 7 6 8 4 9 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "categoric_columns = df.select_dtypes(include='object').columns\n",
    "for col in categoric_columns:\n",
    "    if col == \"stock\":\n",
    "        continue\n",
    "    print(f\"column {col}, data: \\n {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "industry\n",
       "Automobiles & Components                           5\n",
       "Banks                                             15\n",
       "Capital Goods                                     48\n",
       "Commercial & Professional Services                 9\n",
       "Consumer Durables & Apparel                       13\n",
       "Consumer Services                                 15\n",
       "Diversified Financials                            26\n",
       "Energy                                            23\n",
       "Food & Staples Retailing                           4\n",
       "Food, Beverage & Tobacco                          23\n",
       "Health Care Equipment & Services                  37\n",
       "Household & Personal Products                      5\n",
       "Insurance                                         21\n",
       "Materials                                         28\n",
       "Media & Entertainment                             20\n",
       "Pharmaceuticals, Biotechnology & Life Sciences    27\n",
       "Real Estate                                       28\n",
       "Retailing                                         22\n",
       "Semiconductors & Semiconductor Equipment          21\n",
       "Software & Services                               36\n",
       "Technology Hardware & Equipment                   17\n",
       "Telecommunication Services                         3\n",
       "Transportation                                    14\n",
       "Utilities                                         29\n",
       "Name: stock, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of stocks per industry\n",
    "df[['sector','industry','stock']].groupby('industry').stock.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sector\n",
       "Communication Services    23\n",
       "Consumer Discretionary    55\n",
       "Consumer Staples          32\n",
       "Energy                    23\n",
       "Financials                62\n",
       "Health Care               64\n",
       "Industrials               71\n",
       "Information Technology    74\n",
       "Materials                 28\n",
       "Real Estate               28\n",
       "Utilities                 29\n",
       "Name: stock, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of stocks per sector\n",
    "df[['sector','industry','stock']].groupby('sector').stock.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop \"Stocks\" and \"Industry\" columns as there are too many unique values that block us from generalizing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"industry\", \"stock\"], axis = 1, inplace = True)\n",
    "#df.drop([\"industry\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if there are any infinite numbers that can cause same trublesas nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPS_YoY_Growth                51\n",
      "EPS_QoQ_frcst_diff           122\n",
      "EPS_1Y_exp_Change            206\n",
      "EPS_1Y_exp_Change_QoQ      40281\n",
      "future_15dprice_change      5346\n",
      "future_30dprice_change     10692\n",
      "future_60dprice_change     21401\n",
      "future_90dprice_change     32108\n",
      "future_120dprice_change    42781\n",
      "future_150dprice_change    53451\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_1Y_exp_Change.notnull() & df.EPS_YoY_Growth.notnull() & df.EPS_QoQ_frcst_diff.notnull() & df.EPS_1Y_exp_Change_QoQ.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_15dprice_change      5346\n",
      "future_30dprice_change     10692\n",
      "future_60dprice_change     21391\n",
      "future_90dprice_change     32076\n",
      "future_120dprice_change    42746\n",
      "future_150dprice_change    53416\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['Fed_Balance_YoY'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning is Over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the data to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our dataset have the range of dates that are very close to each other. \n",
    "\n",
    "The changes in trading value are rarely very different between today and yesterday. \n",
    "\n",
    "There are exemptions but mostly these are connected with big surprises, and in any case we would notice the change even if we track every other day or every 4th day.\n",
    "\n",
    "It means that removing part of the dataset should help us in building more generalized model since we will be looking for trend but not for matching values.\n",
    "\n",
    "\n",
    "Let's do this, let's remove 4/5 of the dataset.\n",
    "\n",
    "That means we will keep only one day of data per week. It will help us to generalize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Old dataframe shape:\", df.shape)\n",
    "df_compact = df.iloc[::5, :]\n",
    "print(\"New dataframe shape:\", df_compact.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset is 131K rows long with 53 variables and 6 targets(price after 15 days, 30 days, 60 days, 90 days, 120 days and 150 days)\n",
    "\n",
    "We can remove the bigger dataset, but let's save it to the file before doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading/ML_Part/EOD\")\n",
    "df.to_csv(\"full_cleaned_dataframe_2023.csv\", index = False, header = True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' save the shorter version of the dataframe as well so we can get it faster when required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/ML_Part/EOD\")\n",
    "df_compact.to_csv(\"shorter_cleaned_dataframe_2023.csv\", index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
