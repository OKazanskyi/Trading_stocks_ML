{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_core.paths import jupyter_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb # Our ML library\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will run the normal distribution test with\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    TimeSeriesSplit,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can help with the visualization\n",
    "import matplotlib.gridspec as gridspec\n",
    "import SeabornFig2Grid as sfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all CSVs with stocks data and append to one big file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Change target variables calculation in all files\n",
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading/CSVs\")\n",
    "\"\"\"\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/SP500_CSVs_072022\")\n",
    "filepaths = [f for f in os.listdir(\"./\") if f.endswith('.csv')]\n",
    "df = pd.DataFrame()\n",
    "for i in filepaths:\n",
    "    iterate_df = pd.DataFrame()\n",
    "    iterate_df = pd.read_csv(i, encoding= 'unicode_escape')\n",
    "    iterate_df[\"stock\"] = i[:-4]\n",
    "    iterate_df.drop([\"future_15dprice_change\",\"future_30dprice_change\",\"future_60dprice_change\",\"future_90dprice_change\",\"future_120dprice_change\",\"future_150dprice_change\"], axis = 1, inplace = True)\n",
    "    iterate_df.date = pd.to_datetime(iterate_df.date)\n",
    "    iterate_df.sort_values(by = [\"date\"], inplace = True, ascending = False)\n",
    "    iterate_df[\"future_15dprice_change\"] = (iterate_df[\"close\"].shift(11) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df[\"future_30dprice_change\"] = (iterate_df[\"close\"].shift(22) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df[\"future_60dprice_change\"] = (iterate_df[\"close\"].shift(44) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df[\"future_90dprice_change\"] = (iterate_df[\"close\"].shift(66) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df[\"future_120dprice_change\"] = (iterate_df[\"close\"].shift(88) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df[\"future_150dprice_change\"] = (iterate_df[\"close\"].shift(110) / iterate_df[\"close\"]) - 1\n",
    "    iterate_df.to_csv(i, header = True, index = False)\n",
    "#df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading/CSVs\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/SP500_CSVs_012023\")\n",
    "filepaths = [f for f in os.listdir(\"./\") if f.endswith('.csv')]\n",
    "df = pd.DataFrame()\n",
    "for i in filepaths:\n",
    "    iterate_df = pd.DataFrame()\n",
    "    iterate_df = pd.read_csv(i, encoding= 'unicode_escape')\n",
    "    iterate_df[\"stock\"] = i[:-4]\n",
    "    df = pd.concat([df,iterate_df])\n",
    "#df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record the dataframe to speed up the future reading process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/\")\n",
    "df.to_csv(\"THE_FINAL_DATASET_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python\")\n",
    "df= pd.read_csv(\"THE_FINAL_DATASET_2023.csv\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python\")\n",
    "#df.drop([\"Unnamed: 0\",\"dataCode\",\"name\",\"description\",\"statementType\",\"units\",\"Unnamed: 0.2\", \"Unnamed: 0.1\"], axis = 1, inplace = True)\n",
    "df = df[df.close.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YoY_DY'] = 100*df['DY'].pct_change(periods = -365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903111, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Date column as an index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much empty values we have by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_after_earnings_report    219898\n",
      "sector                            35\n",
      "industry                          35\n",
      "LTDE                            4553\n",
      "DE                              1208\n",
      "DPR                           102932\n",
      "Acc_Rec_Pay_Ration              1879\n",
      "DY                            102932\n",
      "PEG_Forward                     2277\n",
      "PEG_Backwards                    128\n",
      "EPS_surprise                  219898\n",
      "EPS_YoY_Growth                219898\n",
      "EPS_QoQ_frcst_diff            219898\n",
      "EPS_1Y_exp_Change             222833\n",
      "YoY_ROE                         9539\n",
      "YoY_LTDE                        9539\n",
      "YoY_DE                          9539\n",
      "YoY_CR                          9539\n",
      "YoY_GM                          9539\n",
      "YoY_ROA                         9539\n",
      "YoY_DPR                       100620\n",
      "YoY_AR_Ration                  10965\n",
      "YoY_ES                          9539\n",
      "YoY_Piotroski                   9539\n",
      "YoY_PE                          9539\n",
      "YoY_PB                          9539\n",
      "YoY_PEGF                        9539\n",
      "YoY_PEGB                        9539\n",
      "future_15dprice_change          5467\n",
      "future_30dprice_change         10931\n",
      "future_60dprice_change         21834\n",
      "future_90dprice_change         32724\n",
      "future_120dprice_change        43614\n",
      "future_150dprice_change        54504\n",
      "VIX_high                         146\n",
      "days_after_crisis             135638\n",
      "VIX_DoD                          497\n",
      "VIX_WoW                         2485\n",
      "VIX_MoM                        10931\n",
      "10YBond                       121943\n",
      "10YB_MoM                      122669\n",
      "10YB_YoY                      130226\n",
      "10YB_30MA_Vector              122669\n",
      "10YB_200MA_Vector             126431\n",
      "10Y_Val_to_30MA               122636\n",
      "10Y_Val_to_200MA              126431\n",
      "YoY_DY                           365\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "zeroes = df.isnull().sum()\n",
    "print(zeroes[zeroes>0])\n",
    "del zeroes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with \"EPS\" column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many null values in the \"current_ratio\" column. Let's see the stocks where it happens and then decide what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AMD', 'AMZN', 'APA', 'APTV', 'AVB', 'AZO', 'BAX', 'BBY', 'BIO',\n",
       "       'BSX', 'CAG', 'CCL', 'CDNS', 'CHD', 'COST', 'CZR', 'DLTR', 'ENPH',\n",
       "       'ETSY', 'FOX', 'GILD', 'HSIC', 'INCY', 'INVH', 'JKHY', 'KHC', 'L',\n",
       "       'LEN', 'MOS', 'NI', 'NOW', 'ORLY', 'PARA', 'PEP', 'PKG', 'PNW',\n",
       "       'RJF', 'ROP', 'ROST', 'TAP', 'TSLA', 'TTWO', 'TXT', 'UNH', 'YUM'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.EPS_surprise.isnull()].stock.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the original files, it's apparently the null values comes from the older data.\n",
    "\n",
    "Before 2017 our API did not provide us info about earinings. \n",
    "\n",
    "We can drop the reocrds with 'nan' values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will remove only 10k records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_surprise.notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_cols(df):\n",
    "    zeroes = df.isnull().sum()\n",
    "    print(zeroes[zeroes>0])\n",
    "    \n",
    "check_null_cols(df)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the null values in the \"dividends\" column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 30% of data is affected by null values in the dividends columns.\n",
    "Let's check how many companies are in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies with zero values in dividends 49\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of companies with zero values in dividends\",len(df[df['DY'].isnull()].stock.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This companies do not pay dividends, we can replace payments to \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DY'].fillna(0, inplace=True)\n",
    "df['YoY_DPR'].fillna(0, inplace=True)\n",
    "df['DPR'].fillna(0, inplace=True)\n",
    "df['YoY_DY'].fillna(0, inplace=True)                   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with YoY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YoY variables with null values are caused by YoY calculation of the rows without historical data.\n",
    "Let's drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.YoY_CR.notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector                        35\n",
      "industry                      35\n",
      "LTDE                        4553\n",
      "DE                          1208\n",
      "Acc_Rec_Pay_Ration          1879\n",
      "PEG_Forward                 2277\n",
      "PEG_Backwards                119\n",
      "EPS_1Y_exp_Change           2935\n",
      "YoY_AR_Ration               1426\n",
      "future_15dprice_change      5027\n",
      "future_30dprice_change     10051\n",
      "future_60dprice_change     20074\n",
      "future_90dprice_change     30084\n",
      "future_120dprice_change    40094\n",
      "future_150dprice_change    50104\n",
      "VIX_DoD                      459\n",
      "VIX_WoW                     2295\n",
      "VIX_MoM                    10095\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673674, 60)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with debt ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the companies that contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FFIV', 'FTNT'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['DE'].isnull()].stock.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems these companies had zero debt for some quarters.\n",
    "\n",
    "Let's replace with \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DE'].fillna(0, inplace=True)\n",
    "df['LTDE'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector                        35\n",
      "industry                      35\n",
      "Acc_Rec_Pay_Ration          1879\n",
      "PEG_Forward                 2277\n",
      "PEG_Backwards                119\n",
      "EPS_1Y_exp_Change           2935\n",
      "YoY_AR_Ration               1426\n",
      "future_15dprice_change      5027\n",
      "future_30dprice_change     10051\n",
      "future_60dprice_change     20074\n",
      "future_90dprice_change     30084\n",
      "future_120dprice_change    40094\n",
      "future_150dprice_change    50104\n",
      "VIX_DoD                      459\n",
      "VIX_WoW                     2295\n",
      "VIX_MoM                    10095\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with VIX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove null values for VIX as these are related to the historical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.VIX_MoM.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector                        13\n",
      "industry                      13\n",
      "Acc_Rec_Pay_Ration          1835\n",
      "PEG_Forward                 2034\n",
      "PEG_Backwards                 61\n",
      "EPS_1Y_exp_Change           2692\n",
      "YoY_AR_Ration               1404\n",
      "future_15dprice_change      5016\n",
      "future_30dprice_change     10023\n",
      "future_60dprice_change     20033\n",
      "future_90dprice_change     30043\n",
      "future_120dprice_change    40053\n",
      "future_150dprice_change    50063\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Accounts Payable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CINF', 'MAA'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Acc_Rec_Pay_Ration'].isnull()].stock.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems these companies had zero Accounts receivables for some quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Acc_Rec_Pay_Ration'].fillna(0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with PEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FOXA', 'LHX', 'PLD', 'SEDG'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['PEG_Forward'].isnull()].stock.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the rows as the effect would be unsignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.PEG_Forward.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PKI', 'SYF'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['PEG_Backwards'].isnull()].stock.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.PEG_Backwards.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_1Y_exp_Change.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector                        13\n",
      "industry                      13\n",
      "YoY_AR_Ration               1404\n",
      "future_15dprice_change      5016\n",
      "future_30dprice_change     10023\n",
      "future_60dprice_change     20033\n",
      "future_90dprice_change     30043\n",
      "future_120dprice_change    40053\n",
      "future_150dprice_change    50063\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEN'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sector'].isnull()].stock.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the sector and industry values for the GEN company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['stock'] == 'GEN','sector'] = 'Information Technology'\n",
    "df.loc[df['stock'] == 'GEN','industry'] = 'Software & Services'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YoY_AR_Ration               1404\n",
      "future_15dprice_change      5016\n",
      "future_30dprice_change     10023\n",
      "future_60dprice_change     20033\n",
      "future_90dprice_change     30043\n",
      "future_120dprice_change    40053\n",
      "future_150dprice_change    50063\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.YoY_AR_Ration.notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can keep only the necessery columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column sector, data: \n",
      " ['Health Care' 'Industrials' 'Consumer Discretionary'\n",
      " 'Information Technology' 'Financials' 'Consumer Staples' 'Utilities'\n",
      " 'Materials' 'Real Estate' 'Energy' 'Communication Services']\n",
      "column industry, data: \n",
      " ['Pharmaceuticals, Biotechnology & Life Sciences' 'Transportation'\n",
      " 'Retailing' 'Technology Hardware & Equipment'\n",
      " 'Health Care Equipment & Services' 'Insurance' 'Software & Services'\n",
      " 'Semiconductors & Semiconductor Equipment' 'Food, Beverage & Tobacco'\n",
      " 'Utilities' 'Materials' 'Capital Goods' 'Diversified Financials'\n",
      " 'Real Estate' 'Energy' 'Automobiles & Components' 'Media & Entertainment'\n",
      " 'Banks' 'Consumer Services' 'Household & Personal Products'\n",
      " 'Food & Staples Retailing' 'Commercial & Professional Services'\n",
      " 'Consumer Durables & Apparel' 'Telecommunication Services']\n"
     ]
    }
   ],
   "source": [
    "categoric_columns = df.select_dtypes(include='object').columns\n",
    "for col in categoric_columns:\n",
    "    if col == \"stock\":\n",
    "        continue\n",
    "    print(f\"column {col}, data: \\n {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "industry\n",
       "Automobiles & Components                           5\n",
       "Banks                                             18\n",
       "Capital Goods                                     48\n",
       "Commercial & Professional Services                 9\n",
       "Consumer Durables & Apparel                       13\n",
       "Consumer Services                                 15\n",
       "Diversified Financials                            26\n",
       "Energy                                            23\n",
       "Food & Staples Retailing                           4\n",
       "Food, Beverage & Tobacco                          22\n",
       "Health Care Equipment & Services                  36\n",
       "Household & Personal Products                      6\n",
       "Insurance                                         23\n",
       "Materials                                         29\n",
       "Media & Entertainment                             20\n",
       "Pharmaceuticals, Biotechnology & Life Sciences    27\n",
       "Real Estate                                       29\n",
       "Retailing                                         22\n",
       "Semiconductors & Semiconductor Equipment          21\n",
       "Software & Services                               35\n",
       "Technology Hardware & Equipment                   17\n",
       "Telecommunication Services                         4\n",
       "Transportation                                    14\n",
       "Utilities                                         29\n",
       "Name: stock, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of stocks per industry\n",
    "df[['sector','industry','stock']].groupby('industry').stock.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sector\n",
       "Communication Services    24\n",
       "Consumer Discretionary    55\n",
       "Consumer Staples          32\n",
       "Energy                    23\n",
       "Financials                67\n",
       "Health Care               63\n",
       "Industrials               71\n",
       "Information Technology    73\n",
       "Materials                 29\n",
       "Real Estate               29\n",
       "Utilities                 29\n",
       "Name: stock, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of stocks per sector\n",
    "df[['sector','industry','stock']].groupby('sector').stock.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop \"Stocks\" and \"Industry\" columns as there are too many unique values that block us from generalizing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"industry\", \"stock\"], axis = 1, inplace = True)\n",
    "#df.drop([\"industry\"], axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if there are any infinite numbers that can cause same trublesas nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPS_YoY_Growth                36\n",
      "EPS_QoQ_frcst_diff           101\n",
      "EPS_1Y_exp_Change            206\n",
      "future_15dprice_change      5005\n",
      "future_30dprice_change     10001\n",
      "future_60dprice_change     19989\n",
      "future_90dprice_change     29977\n",
      "future_120dprice_change    39965\n",
      "future_150dprice_change    49953\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "check_null_cols(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EPS_1Y_exp_Change.notnull() & df.EPS_YoY_Growth.notnull() & df.EPS_QoQ_frcst_diff.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_15dprice_change      5005\n",
      "future_30dprice_change     10001\n",
      "future_60dprice_change     19989\n",
      "future_90dprice_change     29977\n",
      "future_120dprice_change    39965\n",
      "future_150dprice_change    49953\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_null_cols(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning is Over!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the data to avoid overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our dataset have the range of dates that are very close to each other. \n",
    "\n",
    "The changes in trading value are rarely very different between today and yesterday. \n",
    "\n",
    "There are exemptions but mostly these are connected with big surprises, and in any case we would notice the change even if we track every other day or every 4th day.\n",
    "\n",
    "It means that removing part of the dataset should help us in building more generalized model since we will be looking for trend but not for matching values.\n",
    "\n",
    "\n",
    "Let's do this, let's remove 4/5 of the dataset.\n",
    "\n",
    "That means we will keep only one day of data per week. It will help us to generalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old dataframe shape: (659142, 59)\n",
      "New dataframe shape: (131829, 59)\n"
     ]
    }
   ],
   "source": [
    "print(\"Old dataframe shape:\", df.shape)\n",
    "df_compact = df.iloc[::5, :]\n",
    "print(\"New dataframe shape:\", df_compact.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset is 131K rows long with 53 variables and 6 targets(price after 15 days, 30 days, 60 days, 90 days, 120 days and 150 days)\n",
    "\n",
    "We can remove the bigger dataset, but let's save it to the file before doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/Personal-oleg.kazanskyi/Trading Python/ML_Part\")\n",
    "df.to_csv(\"full_cleaned_dataframe_2023.csv\", index = False, header = True)\n",
    "del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' save the shorter version of the dataframe as well so we can get it faster when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compact.to_csv(\"shorter_cleaned_dataframe_2023.csv\", index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
