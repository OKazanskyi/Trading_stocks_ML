{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_core.paths import jupyter_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import lightgbm as lgb # Our ML library\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model_path = \"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python/Models/15days_model.joblib\"\n",
    "lgbm = joblib.load(open(model_path, 'rb'))\n",
    "\n",
    "label_encoder_path = \"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python/Models/label_encoder.joblib\"\n",
    "le = joblib.load(open(label_encoder_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch pulling data cycle of scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 105.0.5195\n",
      "Get LATEST chromedriver version for 105.0.5195 google-chrome\n",
      "Driver [C:\\Users\\oleg.kazanskyi\\.wdm\\drivers\\chromedriver\\win32\\105.0.5195.52\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We remove old csv file with metadata and replace with the newer one\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol     Security SEC filings  GICS Sector         GICS Sub-Industry  \\\n",
      "0    MMM           3M     reports  Industrials  Industrial Conglomerates   \n",
      "1    AOS  A. O. Smith     reports  Industrials         Building Products   \n",
      "2    ABT       Abbott     reports  Health Care     Health Care Equipment   \n",
      "3   ABBV       AbbVie     reports  Health Care           Pharmaceuticals   \n",
      "4   ABMD      Abiomed     reports  Health Care     Health Care Equipment   \n",
      "\n",
      "     Headquarters Location Date first added      CIK      Founded  \n",
      "0    Saint Paul, Minnesota       1976-08-09    66740         1902  \n",
      "1     Milwaukee, Wisconsin       2017-07-26    91142         1916  \n",
      "2  North Chicago, Illinois       1964-03-31     1800         1888  \n",
      "3  North Chicago, Illinois       2012-12-31  1551152  2013 (1888)  \n",
      "4   Danvers, Massachusetts       2018-05-31   815094         1981  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current google-chrome version is 105.0.5195\n",
      "Get LATEST chromedriver version for 105.0.5195 google-chrome\n",
      "Driver [C:\\Users\\oleg.kazanskyi\\.wdm\\drivers\\chromedriver\\win32\\105.0.5195.52\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMG\n",
      "Combining Statements for 2022 and 2\n",
      "Combining Statements for 2022 and 1\n",
      "Combining Statements for 2021 and 4\n",
      "Combining Statements for 2021 and 0\n",
      "Combining Statements for 2021 and 3\n",
      "Combining Statements for 2021 and 2\n",
      "Combining Statements for 2021 and 1\n",
      "Combining Statements for 2020 and 4\n",
      "Combining Statements for 2020 and 0\n",
      "Combining Statements for 2020 and 3\n",
      "Combining Statements for 2020 and 2\n",
      "Combining Statements for 2020 and 1\n",
      "Combining Statements for 2019 and 4\n",
      "Combining Statements for 2019 and 0\n",
      "Combining Statements for 2019 and 3\n",
      "Combining Statements for 2019 and 2\n",
      "Combining Statements for 2019 and 1\n",
      "Combining Statements for 2018 and 4\n",
      "Combining Statements for 2018 and 0\n",
      "Combining Statements for 2018 and 3\n",
      "Combining Statements for 2018 and 2\n",
      "Combining Statements for 2018 and 1\n",
      "Combining Statements for 2017 and 4\n",
      "Combining Statements for 2017 and 0\n",
      "Combining Statements for 2017 and 3\n",
      "Combining Statements for 2017 and 2\n",
      "Combining Statements for 2017 and 1\n",
      "Combining Statements for 2016 and 4\n",
      "Combining Statements for 2016 and 0\n",
      "Combining Statements for 2016 and 3\n",
      "Combining Statements for 2016 and 2\n",
      "Combining Statements for 2016 and 1\n",
      "Combining Statements for 2015 and 4\n",
      "Combining Statements for 2015 and 0\n",
      "Combining Statements for 2015 and 3\n",
      "Combining Statements for 2015 and 2\n",
      "Combining Statements for 2015 and 1\n",
      "Combining Statements for 2014 and 4\n",
      "Combining Statements for 2014 and 0\n",
      "Combining Statements for 2014 and 3\n",
      "Combining Statements for 2014 and 2\n",
      "Combining Statements for 2014 and 1\n",
      "Combining Statements for 2013 and 4\n",
      "Combining Statements for 2013 and 0\n",
      "Combining Statements for 2013 and 3\n",
      "Combining Statements for 2013 and 2\n",
      "Combining Statements for 2013 and 1\n",
      "Combining Statements for 2012 and 4\n",
      "Combining Statements for 2012 and 0\n",
      "Combining Statements for 2012 and 3\n",
      "3\n",
      "4\n",
      "6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StaleElementReferenceException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\OLEG~1.KAZ\\AppData\\Local\\Temp/ipykernel_18464/4283668319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpull_all_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpull_all_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_pulling_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - Danaher\\Documents\\Trading Python\\pull_all_data.py\u001b[0m in \u001b[0;36mstart_pulling_data\u001b[1;34m(historical_dates_range)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mfundamentals\u001b[0m  \u001b[1;33m=\u001b[0m   \u001b[0mtii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_fundamentals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtodays_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistorical_dates_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Financial KPIs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mstatements\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mtii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_statements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtodays_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistorical_dates_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstatements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Danaher\\Documents\\Trading Python\\new_earnings.py\u001b[0m in \u001b[0;36mget_earn_and_dividends\u001b[1;34m(symbol, driver)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.zacks.com/stock/research/CSCO/earnings-calendar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;31m#Search stock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0msearch_symbol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Danaher\\Documents\\Trading Python\\new_earnings.py\u001b[0m in \u001b[0;36msearch_symbol\u001b[1;34m(symbol, driver)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#symbol = \"AXP\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0msearch_symbol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#elem = driver.find_element(By.ID, \"ticker\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StaleElementReferenceException' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python\")\n",
    "import pull_all_data\n",
    "pull_all_data.start_pulling_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all CSVs with stocks data and append to one big file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've uploaded the latest SP500 data. Let's read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading/CSVs\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python/CSVs\")\n",
    "filepaths = [f for f in os.listdir(\"./\") if f.endswith('.csv')]\n",
    "df_validation = pd.DataFrame()\n",
    "for i in filepaths:\n",
    "    iterate_df = pd.DataFrame()\n",
    "    iterate_df = pd.read_csv(i, encoding= 'unicode_escape')\n",
    "    iterate_df[\"stock\"] = i[:-4]\n",
    "    df_validation = pd.concat([df_validation,iterate_df])\n",
    "#df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "#os.chdir(\"/Users/olegkazanskyi/Documents/GitHub/Trading\")\n",
    "os.chdir(\"C:/Users/oleg.kazanskyi/OneDrive - Danaher/Documents/Trading Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.reset_index(inplace= True)\n",
    "df_validation = df_validation.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the \"sector\" column null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.loc[df_validation.stock == 'BF.B', \"sector\"] = 'Consumer Defensive'\n",
    "df_validation.loc[df_validation.stock == 'BRK.B', \"sector\"] = 'Financial Services'\n",
    "df_validation[\"sector\"] = le.transform(df_validation[\"sector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assetsNonCurrent           17406\n",
       "debtCurrent                17406\n",
       "liabilitiesCurrent         17406\n",
       "debtNonCurrent             17406\n",
       "liabilitiesNonCurrent      17406\n",
       "retainedEarnings             473\n",
       "investmentsNonCurrent      17406\n",
       "assetsCurrent              17406\n",
       "investmentsCurrent         17406\n",
       "currentRatio               17406\n",
       "longTermDebtEquity           237\n",
       "shareswaDil                21964\n",
       "industry                     216\n",
       "surprise_%                  1358\n",
       "expected_growth             1358\n",
       "previous_surprise           1583\n",
       "days_after_earn_report      1358\n",
       "dividends_change           20298\n",
       "prev_div_change            20752\n",
       "days_after_divid_report    19282\n",
       "10Y_bond_MoM               10516\n",
       "future_15dprice_change      5258\n",
       "future_30dprice_change     10516\n",
       "future_60dprice_change     21032\n",
       "future_90dprice_change     31548\n",
       "future_120dprice_change    42064\n",
       "future_150dprice_change    52580\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_validation.isna().sum()\n",
    "s.loc [s>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with null values in the currentRatio column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new calculated column\n",
    "df_validation[\"CurrentAssets\"] = (df_validation.acctRec + df_validation.cashAndEq + df_validation.inventory + df_validation.taxAssets)\n",
    "df_validation[\"CurrentLiabilities\"] = (df_validation.acctPay + df_validation.taxLiabilities + df_validation.deposits + df_validation.deferredRev)\n",
    "\n",
    "df_validation = df_validation[df_validation[\"CurrentLiabilities\"] != 0]\n",
    "\n",
    "df_validation[\"CurrentRatio\"] = df_validation[\"CurrentAssets\"]/df_validation[\"CurrentLiabilities\"]\n",
    "\n",
    "df_validation.loc[df_validation['currentRatio'].isnull(),'currentRatio'] = df_validation['CurrentRatio']\n",
    "\n",
    "df_validation.drop([\"CurrentRatio\",\"CurrentAssets\",\"CurrentLiabilities\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.date = pd.to_datetime(df_validation.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the dividends values (Copied all the steps from the data processing stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies that don't pay dividends in S&P500: 98\n",
      "['AAL', 'ABMD', 'ADBE', 'ADSK', 'AKAM', 'ALGN', 'ALK', 'AMD', 'AMZN', 'ANET', 'ANSS', 'APTV', 'AZO', 'BA', 'BIIB', 'BIO', 'BKNG', 'BSX', 'CBRE', 'CCL', 'CDAY', 'CDNS', 'CHTR', 'CMG', 'CNC', 'CPRT', 'CRL', 'CRM', 'CTLT', 'CZR', 'DAL', 'DIS', 'DISH', 'DLTR', 'DVA', 'DXC', 'DXCM', 'ENPH', 'EPAM', 'ETSY', 'EW', 'EXPE', 'FFIV', 'FISV', 'FLT', 'FTNT', 'GNRC', 'HOLX', 'HSIC', 'IDXX', 'ILMN', 'INCY', 'IQV', 'ISRG', 'IT', 'KEYS', 'KMX', 'LUV', 'LVS', 'LYV', 'META', 'MHK', 'MNST', 'MRNA', 'MTCH', 'MTD', 'NCLH', 'NFLX', 'NOW', 'NVR', 'ON', 'ORLY', 'PAYC', 'PENN', 'PTC', 'PYPL', 'QRVO', 'RCL', 'REGN', 'SEDG', 'SIVB', 'SNPS', 'TDY', 'TMUS', 'TRMB', 'TSLA', 'TTWO', 'TWTR', 'TYL', 'UAL', 'ULTA', 'URI', 'VRSN', 'VRTX', 'WAT', 'WBD', 'WDC', 'WYNN']\n",
      "Here are the companies with no dividends data \n",
      " \n",
      " ['TDG']\n"
     ]
    }
   ],
   "source": [
    "#Let's see how many companies do not pay dividends at all in the list of SP500\n",
    "\n",
    "dividends_paid_zero = df_validation[[\"divCash\",\"stock\"]].groupby(\"stock\").sum()\n",
    "dividends_paid_zero = dividends_paid_zero[dividends_paid_zero.divCash == 0]\n",
    "list_of_companies_no_divid = list(dividends_paid_zero.index)\n",
    "print(\"Number of companies that don't pay dividends in S&P500:\",len(dividends_paid_zero.index))\n",
    "print(list_of_companies_no_divid)\n",
    "dividends_paid_zero.reset_index(inplace = True)\n",
    "\n",
    "df_validation['dividends_change_calc'] = df_validation.apply(\n",
    "        lambda row: 0 if row['stock'] in list_of_companies_no_divid else row['dividends_change'],\n",
    "        axis=1\n",
    "    )\n",
    "        \n",
    "df_validation['prev_div_change_calc'] = df_validation.apply(\n",
    "        lambda row: 0 if row['stock'] in list_of_companies_no_divid else row['prev_div_change'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "min_date = df_validation.date.min()\n",
    "\n",
    "df_validation['days_after_divid_report_calc'] = df_validation.apply(\n",
    "        lambda row: row['date'] - min_date if row['stock'] in list_of_companies_no_divid else row['days_after_divid_report'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_validation.drop([\"dividends_change\",\"prev_div_change\",\"days_after_divid_report\"], axis = 1, inplace = True)\n",
    "df_validation.rename(columns={\"dividends_change_calc\": \"dividends_change\", \"prev_div_change_calc\": \"prev_div_change\",'days_after_divid_report_calc':\"days_after_divid_report\"}, inplace = True)\n",
    "\n",
    "#Let's see how many rows by a stock may be affected by estimates zero values\n",
    "total_rows_by_stock = df_validation[[\"stock\",\"close\"]].groupby(\"stock\").count().sort_values(by = \"close\", ascending = False)\n",
    "total_rows_by_stock.rename(columns = {\"close\": \"rows_total\"}, inplace = True)\n",
    "\n",
    "deal_with_div = df_validation[df_validation['dividends_change'].isnull()][[\"stock\",\"close\"]].groupby(\"stock\").count().sort_values(by = \"close\", ascending = False)\n",
    "deal_with_div.rename(columns = {\"close\": \"rows_with_nulls\"}, inplace = True)\n",
    "\n",
    "deal_with_div = deal_with_div.join(total_rows_by_stock, how = 'left')\n",
    "deal_with_div[\"% of rows\"] = round(100 * deal_with_div[\"rows_with_nulls\"] / deal_with_div[\"rows_total\"],1)\n",
    "companies_with_no_div_data = deal_with_div[deal_with_div[\"% of rows\"] == 100].index\n",
    "print(\"Here are the companies with no dividends data \\n \\n\", list(companies_with_no_div_data))\n",
    "\n",
    "\n",
    "paydiv_by_stock_zero_dev = df_validation[df_validation.stock.isin(companies_with_no_div_data)][[\"payDiv\",\"stock\"]].groupby(\"stock\").sum()\n",
    "paydiv_by_stock_zero_dev = paydiv_by_stock_zero_dev[paydiv_by_stock_zero_dev.payDiv == 0]\n",
    "paydiv_by_stock_zero_dev\n",
    "\n",
    "df_validation[df_validation.stock.isin(companies_with_no_div_data)][[\"divCash\",\"stock\"]].groupby(\"stock\").sum()\n",
    "paydiv_by_stock_zero_dev = list(paydiv_by_stock_zero_dev.index)\n",
    "\n",
    "\n",
    " # Replace Zeroes for the dividends columns for the companies that do not pay dividends.\n",
    "    # That will help us to see how severe the issue is\n",
    "df_validation['dividends_change_calc'] = df_validation.apply(\n",
    "        lambda row: 0 if row['stock'] in paydiv_by_stock_zero_dev else row['dividends_change'],\n",
    "        axis=1\n",
    "    )\n",
    "        \n",
    "df_validation['prev_div_change_calc'] = df_validation.apply(\n",
    "        lambda row: 0 if row['stock'] in paydiv_by_stock_zero_dev else row['prev_div_change'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_validation['days_after_divid_report_calc'] = df_validation.apply(\n",
    "        lambda row: row['date'] - min_date if row['stock'] in list_of_companies_no_divid else row['days_after_divid_report'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_validation.drop([\"dividends_change\",\"prev_div_change\",\"days_after_divid_report\"], axis = 1, inplace = True)\n",
    "df_validation.rename(columns={\"dividends_change_calc\": \"dividends_change\", \"prev_div_change_calc\": \"prev_div_change\",\"days_after_divid_report_calc\":\"days_after_divid_report\"}, inplace = True)\n",
    "\n",
    "paydiv_by_stock = df_validation[df_validation.stock.isin(companies_with_no_div_data)][[\"payDiv\",\"stock\"]].groupby(\"stock\").sum()\n",
    "paydiv_by_stock = list(paydiv_by_stock.index)\n",
    "paydiv_by_stock.index\n",
    "\n",
    "paydiv_by_stock = [ele for ele in paydiv_by_stock if ele not in paydiv_by_stock_zero_dev]\n",
    "paydiv_by_stock\n",
    "\n",
    "from datetime import timedelta\n",
    "#Step 1\n",
    "df_div = df_validation[df_validation.stock.isin(paydiv_by_stock)][[\"date\",\"payDiv\",\"sharesBasic\",\"stock\"]]\n",
    "df_div[\"div_per_stock\"] = df_div.payDiv/df_div.sharesBasic\n",
    "\n",
    "#Step2\n",
    "df_div[\"dividends_change_calc\"]  = df_div.div_per_stock-df_div.div_per_stock.shift(-1)\n",
    "df_div[\"prev_stock\"] = df_div.stock.shift(-1)\n",
    "\n",
    "#Step3\n",
    "df_div = df_div[(df_div.dividends_change_calc != 0) & (df_div.stock == df_div.prev_stock)]\n",
    "df_div[\"date\"] = pd.to_datetime(df_div[\"date\"])\n",
    "df_div[\"date_announced\"] = df_div[\"date\"]-timedelta(days=60)\n",
    "df_div[\"date_of_change\"] = df_div[\"date_announced\"]\n",
    "df_div.drop(\"date\", axis = 1, inplace = True)\n",
    "df_div.rename(columns={\"date_announced\":\"date\"}, inplace = True)\n",
    "df_div.set_index([\"date\"], inplace = True)\n",
    "\n",
    "#Step4dates_df\n",
    "df_div[\"prev_div_change_calc\"] = df_div[\"dividends_change_calc\"].shift(-1)\n",
    "\n",
    "#Step5\n",
    "dates_df=pd.DataFrame()\n",
    "dates_df[\"date\"] = pd.date_range(start=date.today()-timedelta(days=4000), end=date.today())\n",
    "dates_df.set_index([\"date\"], inplace = True)\n",
    "\n",
    "df_div_full = pd.DataFrame()\n",
    "\n",
    "for symbol in df_div.stock.unique():\n",
    "    df_div_part = dates_df.join(df_div[df_div.stock == symbol], how = 'left')\n",
    "    df_div_part.sort_values(by = 'date', axis = 0, ascending = True, inplace = True)\n",
    "    df_div_part.ffill(axis = 0, inplace = True)\n",
    "    df_div_part.sort_values(by = 'date', axis = 0, ascending = False, inplace = True)\n",
    "    df_div_part[\"days_after_divid_report_calc\"] = df_div_part.index - df_div_part.date_of_change\n",
    "    df_div_part = df_div_part[df_div_part[\"days_after_divid_report_calc\"].notnull()]\n",
    "    df_div_part[\"days_after_divid_report_calc\"] = df_div_part[\"days_after_divid_report_calc\"].dt.days.astype('int16')\n",
    "    df_div_full = pd.concat([df_div_full,df_div_part])\n",
    "\n",
    "df_div_full = df_div_full[[\"stock\", \"dividends_change_calc\", \"prev_div_change_calc\", \"days_after_divid_report_calc\"]]\n",
    "df_div_full.reset_index(inplace = True)\n",
    "df_div_full = df_div_full[df_div_full.stock.notnull()]\n",
    "\n",
    "#step6\n",
    "df_validation.date = pd.to_datetime(df_validation.date)\n",
    "df_validation = pd.merge(df_validation, df_div_full, on = ['date','stock'], how = \"left\")\n",
    "\n",
    "# Replace Zeroes for the dividends columns for the companies that do not pay dividends.\n",
    "    # That will help us to see how severe the issue is\n",
    "df_validation['dividends_change_final'] = df_validation.apply(\n",
    "        lambda row: row[\"dividends_change_calc\"] if row['stock'] in paydiv_by_stock else row['dividends_change'],\n",
    "        axis=1\n",
    "    )\n",
    "        \n",
    "df_validation['prev_div_change_final'] = df_validation.apply(\n",
    "        lambda row: row[\"prev_div_change_calc\"] if row['stock'] in paydiv_by_stock else row['prev_div_change'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_validation['days_after_divid_report_final'] = df_validation.apply(\n",
    "        lambda row: row[\"days_after_divid_report_calc\"] if row['stock'] in paydiv_by_stock else row['days_after_divid_report'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_validation.drop([\"dividends_change\",\"prev_div_change\", \"dividends_change_calc\", \"prev_div_change_calc\",\"days_after_divid_report_calc\", \"days_after_divid_report\"], axis = 1, inplace = True)\n",
    "\n",
    "df_validation.rename(columns={\"dividends_change_final\": \"dividends_change\", \"prev_div_change_final\": \"prev_div_change\", \"days_after_divid_report_final\": \"days_after_divid_report\"}, inplace = True)\n",
    "\n",
    "\n",
    "df_validation = df_validation[df_validation[\"prev_div_change\"].notnull()]\n",
    "df_validation = df_validation[df_validation[\"days_after_divid_report\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all datatypes inside the days_after_divid_report to int\n",
    "\n",
    "for i, j in enumerate(df_validation.days_after_divid_report):\n",
    "    if isinstance(j,int):\n",
    "        continue\n",
    "    elif isinstance(j,float):\n",
    "        df_validation.days_after_divid_report.iloc[i] = int(df_validation.days_after_divid_report.iloc[i])\n",
    "    else:\n",
    "        df_validation.days_after_divid_report.iloc[i] = df_validation.days_after_divid_report.iloc[i].days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_validation[df_validation[\"previous_surprise\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dealing with \"longTermDebtEquity\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies with some zero values in longTermDebtEquity:  1\n",
      "\n",
      " Here are the companies\n",
      " <StringArray>\n",
      "['BEN']\n",
      "Length: 1, dtype: string\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of companies with some zero values in longTermDebtEquity: \",len(df_validation[df_validation['longTermDebtEquity'].isnull()].stock.unique()))\n",
    "print(\"\\n Here are the companies\\n\",df_validation[df_validation['longTermDebtEquity'].isnull()].stock.unique())\n",
    "companies_null_longdebt = list(df_validation[df_validation['longTermDebtEquity'].isnull()].stock.unique())\n",
    "\n",
    "df_validation.loc[df_validation.stock.isin(companies_null_longdebt),'longTermDebtEquity'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with 10Y MoM null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_validation[df_validation[\"10Y_bond_MoM\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assetsNonCurrent           14468\n",
       "debtCurrent                14468\n",
       "liabilitiesCurrent         14468\n",
       "debtNonCurrent             14468\n",
       "liabilitiesNonCurrent      14468\n",
       "retainedEarnings             429\n",
       "investmentsNonCurrent      14468\n",
       "assetsCurrent              14468\n",
       "investmentsCurrent         14468\n",
       "shareswaDil                18532\n",
       "industry                     194\n",
       "future_15dprice_change      5082\n",
       "future_30dprice_change     10164\n",
       "future_60dprice_change     20328\n",
       "future_90dprice_change     30509\n",
       "future_120dprice_change    40699\n",
       "future_150dprice_change    50919\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_validation.isna().sum()\n",
    "s.loc [s>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to keep only the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-08'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today() \n",
    "historical_date = today-timedelta(days=1)\n",
    "historical_date = historical_date.strftime(\"%Y-%m-%d\") \n",
    "historical_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.date = pd.to_datetime(df_validation.date)\n",
    "df_validation = df_validation[df_validation.date>=historical_date]\n",
    "df_validation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = df_validation[\"stock\"]\n",
    "stocks = pd.Series(stocks)\n",
    "date = df_validation[\"date\"]\n",
    "date = pd.Series(date)\n",
    "df_validation = df_validation[['roe', 'longTermDebtEquity', 'revenueQoQ', 'epsQoQ',\n",
    "       'piotroskiFScore', 'currentRatio', 'roa', 'profitMargin', 'peRatio',\n",
    "       'pbRatio', 'VIX_high', 'sector', '10Y_bonds', '10Y_bond_MoM',\n",
    "       'Debt-to-Equity_Ratio', 'DividendsYield', 'PayoutRatio',\n",
    "       'Acc_Rec_Pay_Ration', 'Earnings_per_stock', 'dividends_change',\n",
    "       'prev_div_change', 'days_after_divid_report', 'surprise_%',\n",
    "       'expected_growth', 'previous_surprise', 'days_after_earn_report']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only the newest unseen records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating X and y validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_num_var = ['roe', 'longTermDebtEquity', 'revenueQoQ', 'epsQoQ', 'piotroskiFScore', 'currentRatio', 'roa', 'profitMargin', 'peRatio', 'pbRatio', 'VIX_high', '10Y_bonds', '10Y_bond_MoM', 'Debt-to-Equity_Ratio', 'DividendsYield', 'PayoutRatio', 'Acc_Rec_Pay_Ration', 'Earnings_per_stock', 'dividends_change', 'prev_div_change', 'days_after_divid_report', 'surprise_%', 'expected_growth', 'previous_surprise', 'days_after_earn_report']\n",
    "\n",
    "for i in list_of_num_var:\n",
    "    df_validation[i] = df_validation[i].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the values.\n",
    "\n",
    "We saved our classification models in the list list_of_classification_models.\n",
    "\n",
    "At first for the 15 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predicting the values\n",
    "ypred_15=lgbm.predict(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.reset_index(inplace = True)\n",
    "ypred_15 = pd.Series(ypred_15)\n",
    "df_validation = df_validation.join(ypred_15.rename('15d_forecast'), how = 'left')\n",
    "df_validation = df_validation.join(stocks.rename('stock'), how = 'left')\n",
    "df_validation = df_validation.join(date.rename('date'), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_validation[df_validation['15d_forecast'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>roe</th>\n",
       "      <th>longTermDebtEquity</th>\n",
       "      <th>revenueQoQ</th>\n",
       "      <th>epsQoQ</th>\n",
       "      <th>piotroskiFScore</th>\n",
       "      <th>currentRatio</th>\n",
       "      <th>roa</th>\n",
       "      <th>profitMargin</th>\n",
       "      <th>peRatio</th>\n",
       "      <th>...</th>\n",
       "      <th>dividends_change</th>\n",
       "      <th>prev_div_change</th>\n",
       "      <th>days_after_divid_report</th>\n",
       "      <th>surprise_%</th>\n",
       "      <th>expected_growth</th>\n",
       "      <th>previous_surprise</th>\n",
       "      <th>days_after_earn_report</th>\n",
       "      <th>15d_forecast</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, roe, longTermDebtEquity, revenueQoQ, epsQoQ, piotroskiFScore, currentRatio, roa, profitMargin, peRatio, pbRatio, VIX_high, sector, 10Y_bonds, 10Y_bond_MoM, Debt-to-Equity_Ratio, DividendsYield, PayoutRatio, Acc_Rec_Pay_Ration, Earnings_per_stock, dividends_change, prev_div_change, days_after_divid_report, surprise_%, expected_growth, previous_surprise, days_after_earn_report, 15d_forecast, stock, date]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from smtplib import SMTP\n",
    "import smtplib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "mail_content = '''Hello,\n",
    "This is a simple mail. There is only text, no attachments are there The mail is sent using Python SMTP library.\n",
    "Thank You\n",
    "'''\n",
    "#The mail addresses and password\n",
    "sender_address = 'oleg.kazanskyi@gmail.com'\n",
    "sender_pass = input()\n",
    "receiver_address = 'oleg.kazanskyi@gmail.com'\n",
    "#Setup the MIME\n",
    "message = MIMEMultipart()\n",
    "message['From'] = sender_address\n",
    "message['To'] = receiver_address\n",
    "message['Subject'] = 'A test mail sent by Python. It has an attachment.'   #The subject line\n",
    "#The body and the attachments for the mail\n",
    "message.attach(MIMEText(mail_content, 'plain'))\n",
    "#Create SMTP session for sending the mail\n",
    "session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port\n",
    "session.starttls() #enable security\n",
    "session.login(sender_address, sender_pass) #login with mail_id and password\n",
    "text = message.as_string()\n",
    "session.sendmail(sender_address, receiver_address, text)\n",
    "session.quit()\n",
    "print('Mail Sent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipients = ['oleg.kazanskyi@gmail.com'] \n",
    "emaillist = [elem.strip().split(',') for elem in recipients]\n",
    "msg = MIMEMultipart()\n",
    "msg['Subject'] = \"Your Subject\"\n",
    "msg['From'] = 'oleg.kazanskyi@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\\\n",
    "<html>\n",
    "  <head></head>\n",
    "  <body>\n",
    "    {0}\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\".format(df_validation.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = MIMEText(html, 'html')\n",
    "msg.attach(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = smtplib.SMTP('smtp.gmail.com', 587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.sendmail(msg['From'], emaillist , msg.as_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
